{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% [markdown]\n",
    "# # Part 1: BBC News Classification Using Matrix Factorization\n",
    "#\n",
    "# **Files:**\n",
    "# - bbc_news_train.csv (1490 records: ArticleId, Article, Category)\n",
    "# - bbc_news_test.csv (736 records: ArticleId, Text)\n",
    "# - bbc_news_sample_solution.csv (sample submission format)\n",
    "#\n",
    "# **Objective:**\n",
    "# - Build a system to classify unseen news articles.\n",
    "# - Use matrix factorization (NMF) as an unsupervised step to extract latent topics from text and then classify.\n",
    "# - Compare with a supervised approach (direct TF-IDF).\n",
    "# - Experiment with hyperparameters and data efficiency.\n",
    "# - Generate a submission file with predictions.\n",
    "\n",
    "# %% [code]\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "aeab4a4289c0cabc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% [markdown]\n",
    "# ## 1. Data Loading and Train-Validation Split\n",
    "#\n",
    "# Load the training data from **bbc_news_train.csv**.\n",
    "# The file contains: ArticleId, Article, Category.\n",
    "# For internal evaluation, we split into training and validation sets.\n",
    "\n",
    "# %% [code]\n",
    "# Load training data\n",
    "bbc_train = pd.read_csv('.data/bbc_news_train.csv')\n",
    "print(\"Training data shape:\", bbc_train.shape)\n",
    "print(bbc_train.head())\n",
    "\n",
    "# Stratified split (80% train, 20% validation)\n",
    "train_df, val_df = train_test_split(bbc_train, test_size=0.2, random_state=42, stratify=bbc_train['Category'])\n",
    "print(\"Train set shape:\", train_df.shape)\n",
    "print(\"Validation set shape:\", val_df.shape)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Exploratory Data Analysis (EDA)\n",
    "#\n",
    "# Visualize the category distribution in the training set.\n",
    "\n",
    "# %% [code]\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='Category', data=train_df, order=train_df['Category'].value_counts().index)\n",
    "plt.title(\"Category Distribution in Training Data\")\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ],
   "id": "9d5b9e072e47aeb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% [markdown]\n",
    "# ## 3. Feature Extraction with TF-IDF\n",
    "#\n",
    "# **Note:** Fit the vectorizer on the training data (to avoid data leakage) and transform the validation set.\n",
    "# The text column in training data is \"Article\".\n",
    "\n",
    "# %% [code]\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X_train = tfidf_vectorizer.fit_transform(train_df['Article'])\n",
    "X_val = tfidf_vectorizer.transform(val_df['Article'])\n",
    "\n",
    "print(\"TF-IDF Train shape:\", X_train.shape)\n",
    "print(\"TF-IDF Validation shape:\", X_val.shape)\n"
   ],
   "id": "33fba87f31ae4a56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% [markdown]\n",
    "# ## 4. Unsupervised Approach: NMF + Logistic Regression\n",
    "#\n",
    "# For various numbers of latent topics (n_components), we:\n",
    "# - Fit NMF on the training TF-IDF features.\n",
    "# - Transform both training and validation data.\n",
    "# - Train Logistic Regression on the NMF features.\n",
    "# - Record training and validation accuracies.\n",
    "\n",
    "# %% [code]\n",
    "n_components_list = [5, 10, 15, 20]\n",
    "nmf_results = []\n",
    "\n",
    "for n in n_components_list:\n",
    "    nmf = NMF(n_components=n, random_state=42, init='nndsvda', max_iter=300)\n",
    "    W_train = nmf.fit_transform(X_train)\n",
    "    W_val = nmf.transform(X_val)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=500)\n",
    "    clf.fit(W_train, train_df['Category'])\n",
    "\n",
    "    train_pred = clf.predict(W_train)\n",
    "    val_pred = clf.predict(W_val)\n",
    "\n",
    "    train_acc = accuracy_score(train_df['Category'], train_pred)\n",
    "    val_acc = accuracy_score(val_df['Category'], val_pred)\n",
    "\n",
    "    nmf_results.append({'n_components': n, 'Train Accuracy': train_acc, 'Validation Accuracy': val_acc})\n",
    "\n",
    "nmf_results_df = pd.DataFrame(nmf_results)\n",
    "print(\"NMF Hyperparameter Results:\")\n",
    "print(nmf_results_df)\n"
   ],
   "id": "4eca37483310011c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(nmf_results_df['n_components'], nmf_results_df['Train Accuracy'], marker='o', label='Train Accuracy')\n",
    "plt.plot(nmf_results_df['n_components'], nmf_results_df['Validation Accuracy'], marker='o', label='Validation Accuracy')\n",
    "plt.xlabel('Number of NMF Components')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('NMF Hyperparameter Tuning')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "bd90893c284bd6fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% [markdown]\n",
    "# ### Detailed Classification Report using NMF Features (n_components = 10)\n",
    "#\n",
    "# We choose n_components = 10 and display full classification reports.\n",
    "\n",
    "# %% [code]\n",
    "nmf_final = NMF(n_components=10, random_state=42, init='nndsvda', max_iter=300)\n",
    "W_train_final = nmf_final.fit_transform(X_train)\n",
    "W_val_final = nmf_final.transform(X_val)\n",
    "\n",
    "clf_nmf_final = LogisticRegression(max_iter=500)\n",
    "clf_nmf_final.fit(W_train_final, train_df['Category'])\n",
    "\n",
    "train_pred_final = clf_nmf_final.predict(W_train_final)\n",
    "val_pred_final = clf_nmf_final.predict(W_val_final)\n",
    "\n",
    "print(\"Classification Report (NMF features) - Train:\")\n",
    "print(classification_report(train_df['Category'], train_pred_final))\n",
    "print(\"Classification Report (NMF features) - Validation:\")\n",
    "print(classification_report(val_df['Category'], val_pred_final))\n"
   ],
   "id": "2082ac112408f59b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Supervised Approach: TF-IDF + Logistic Regression\n",
    "#\n",
    "# Here we train a classifier directly on the TF-IDF features.\n",
    "\n",
    "# %% [code]\n",
    "clf_tfidf = LogisticRegression(max_iter=500)\n",
    "clf_tfidf.fit(X_train, train_df['Category'])\n",
    "\n",
    "train_pred_tfidf = clf_tfidf.predict(X_train)\n",
    "val_pred_tfidf = clf_tfidf.predict(X_val)\n",
    "\n",
    "print(\"Classification Report (TF-IDF) - Train:\")\n",
    "print(classification_report(train_df['Category'], train_pred_tfidf))\n",
    "print(\"Classification Report (TF-IDF) - Validation:\")\n",
    "print(classification_report(val_df['Category'], val_pred_tfidf))\n"
   ],
   "id": "71a96a8a4bccba65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% [markdown]\n",
    "# ## 6. Data Efficiency Analysis\n",
    "#\n",
    "# We evaluate performance when training on only a fraction of the training data (10%, 20%, 50%, 100%).\n",
    "# Both the unsupervised (NMF+LR) and supervised (TF-IDF+LR) approaches are compared.\n",
    "\n",
    "# %% [code]\n",
    "fractions = [0.1, 0.2, 0.5, 1.0]\n",
    "efficiency_results = []\n",
    "\n",
    "for frac in fractions:\n",
    "    sub_train = train_df.sample(frac=frac, random_state=42)\n",
    "\n",
    "    tfidf_sub = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "    X_sub = tfidf_sub.fit_transform(sub_train['Article'])\n",
    "    X_val_sub = tfidf_sub.transform(val_df['Article'])\n",
    "\n",
    "    # Unsupervised approach\n",
    "    nmf_sub = NMF(n_components=10, random_state=42, init='nndsvda', max_iter=300)\n",
    "    W_sub = nmf_sub.fit_transform(X_sub)\n",
    "    W_val_sub = nmf_sub.transform(X_val_sub)\n",
    "\n",
    "    clf_nmf_sub = LogisticRegression(max_iter=500)\n",
    "    clf_nmf_sub.fit(W_sub, sub_train['Category'])\n",
    "    val_acc_nmf = accuracy_score(val_df['Category'], clf_nmf_sub.predict(W_val_sub))\n",
    "\n",
    "    # Supervised approach\n",
    "    clf_tfidf_sub = LogisticRegression(max_iter=500)\n",
    "    clf_tfidf_sub.fit(X_sub, sub_train['Category'])\n",
    "    val_acc_tfidf = accuracy_score(val_df['Category'], clf_tfidf_sub.predict(X_val_sub))\n",
    "\n",
    "    efficiency_results.append({\n",
    "        'Fraction': frac,\n",
    "        'Unsupervised (NMF) Acc': val_acc_nmf,\n",
    "        'Supervised (TF-IDF) Acc': val_acc_tfidf\n",
    "    })\n",
    "\n",
    "efficiency_df = pd.DataFrame(efficiency_results)\n",
    "print(\"Data Efficiency Analysis:\")\n",
    "print(efficiency_df)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(efficiency_df['Fraction'], efficiency_df['Unsupervised (NMF) Acc'], marker='o', label='Unsupervised (NMF)')\n",
    "plt.plot(efficiency_df['Fraction'], efficiency_df['Supervised (TF-IDF) Acc'], marker='o', label='Supervised (TF-IDF)')\n",
    "plt.xlabel('Fraction of Training Data')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Data Efficiency Analysis')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "1d293624e3e13b80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% [markdown]\n",
    "# ## 7. Final Test Set Prediction and Submission File\n",
    "#\n",
    "# Now load **bbc_news_test.csv** (columns: ArticleId and Text), predict the Category using the chosen model, and create a submission file.\n",
    "\n",
    "# %% [code]\n",
    "# Load test data\n",
    "bbc_test = pd.read_csv('week4_classification/data/bbc_news_test.csv')\n",
    "print(\"Test data shape:\", bbc_test.shape)\n",
    "print(bbc_test.head())\n"
   ],
   "id": "10a6469b2b8cb32e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use the supervised TF-IDF classifier for final prediction.\n",
    "X_test_final = tfidf_vectorizer.transform(bbc_test['Text'])\n",
    "test_predictions = clf_tfidf.predict(X_test_final)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ArticleId': bbc_test['ArticleId'],\n",
    "    'Category': test_predictions\n",
    "})\n",
    "print(\"Submission preview:\")\n",
    "print(submission.head())\n"
   ],
   "id": "22254c672498612e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save submission file (e.g., bbc_news_submission.csv)\n",
    "submission.to_csv('bbc_news_submission.csv', index=False)\n",
    "print(\"Submission file 'bbc_news_submission.csv' created.\")\n"
   ],
   "id": "6393ae1013befb0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% [markdown]\n",
    "# # Part 2: Movie Ratings Prediction with sklearn’s NMF\n",
    "#\n",
    "# **Files:**\n",
    "# - **train.csv:** Contains movie ratings training data with columns: uID, mID, rating\n",
    "# - **test.csv:** Contains movie ratings test data with columns: uID, mID, rating\n",
    "# - **movies.csv:** Contains movie details (mID, title, year, etc.)\n",
    "# - **users.csv:** Contains user information (uID, gender, age, occupation, zip)\n",
    "#\n",
    "# **Objective:**\n",
    "# - Build a recommender system using matrix factorization (NMF) to predict ratings.\n",
    "# - Create a user-item matrix from the training data.\n",
    "# - Use sklearn’s NMF to predict missing ratings for the test set pairs.\n",
    "# - Compute RMSE on the test ratings.\n",
    "# - Compare against a simple baseline (global mean).\n",
    "# - Discuss limitations and potential improvements.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Load the Movie Ratings Data and Build User-Item Matrix\n",
    "\n",
    "# %% [code]\n",
    "# Load the four files (adjust paths as necessary)\n",
    "ratings_train = pd.read_csv('./movie_ratings/train.csv')   # Columns: uID, mID, rating\n",
    "ratings_test  = pd.read_csv('./movie_ratings/test.csv')      # Columns: uID, mID, rating\n",
    "movies = pd.read_csv('./movie_ratings/movies.csv')           # Contains mID, title, year, etc.\n",
    "users  = pd.read_csv('./movie_ratings/users.csv')            # Contains uID, gender, age, occupation, zip\n",
    "\n",
    "print(\"Ratings Train shape:\", ratings_train.shape)\n",
    "print(\"Ratings Test shape:\", ratings_test.shape)\n",
    "print(\"Movies shape:\", movies.shape)\n",
    "print(\"Users shape:\", users.shape)\n",
    "\n",
    "# Create a user-item matrix from the training ratings.\n",
    "# Rows: unique uID; Columns: unique mID; Values: rating\n",
    "user_item_train = ratings_train.pivot(index='uID', columns='mID', values='rating')\n",
    "print(\"User-Item Matrix shape:\", user_item_train.shape)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Prepare the Training Matrix for NMF\n",
    "#\n",
    "# Fill missing ratings with 0. (Note: 0 is not an actual rating but is used for factorization.)\n",
    "\n",
    "# %% [code]\n",
    "train_matrix = user_item_train.fillna(0)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Apply NMF and Predict Ratings on the Test Set\n",
    "#\n",
    "# We experiment with different numbers of latent factors (n_components).\n",
    "# For each setting, we factorize the training matrix, reconstruct it, and then for each (uID, mID) in the test set, we predict the rating.\n",
    "# Finally, we compute the RMSE.\n",
    "\n",
    "# %% [code]\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "n_components_list_ratings = [10, 20, 30, 40]\n",
    "nmf_results_ratings = []\n",
    "\n",
    "for n in n_components_list_ratings:\n",
    "    nmf_model = NMF(n_components=n, init='nndsvda', random_state=42, max_iter=300)\n",
    "    W = nmf_model.fit_transform(train_matrix)\n",
    "    H = nmf_model.components_\n",
    "\n",
    "    reconstructed = np.dot(W, H)\n",
    "\n",
    "    test_actual = []\n",
    "    test_pred = []\n",
    "    for index, row in ratings_test.iterrows():\n",
    "        user = row['uID']\n",
    "        movie = row['mID']\n",
    "        actual_rating = row['rating']\n",
    "        # Get indices in the user-item matrix\n",
    "        if user in train_matrix.index and movie in train_matrix.columns:\n",
    "            u_idx = train_matrix.index.get_loc(user)\n",
    "            m_idx = train_matrix.columns.get_loc(movie)\n",
    "            pred_rating = reconstructed[u_idx, m_idx]\n",
    "            test_actual.append(actual_rating)\n",
    "            test_pred.append(pred_rating)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(test_actual, test_pred))\n",
    "    nmf_results_ratings.append({'n_components': n, 'RMSE': rmse})\n",
    "    print(f\"n_components = {n} -> RMSE: {rmse:.4f}\")\n",
    "\n",
    "nmf_results_ratings_df = pd.DataFrame(nmf_results_ratings)\n",
    "print(\"\\nNMF RMSE Results for Movie Ratings:\")\n",
    "print(nmf_results_ratings_df)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(nmf_results_ratings_df['n_components'], nmf_results_ratings_df['RMSE'], marker='o')\n",
    "plt.xlabel('Number of NMF Components')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('NMF Hyperparameter Tuning for Movie Ratings Prediction')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "e07247b3036e2aee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% [markdown]\n",
    "# ## 4. Baseline Predictor: Global Mean Rating\n",
    "#\n",
    "# We compute the global mean rating from the training data (ignoring zeros) and use it as a prediction for all test entries.\n",
    "# Then compute RMSE for this baseline.\n",
    "\n",
    "# %% [code]\n",
    "# Compute global mean from actual (nonzero) ratings in the training set\n",
    "nonzero_ratings = train_matrix[train_matrix != 0].stack()\n",
    "global_mean = nonzero_ratings.mean()\n",
    "print(\"Global Mean Rating:\", global_mean)\n",
    "\n",
    "baseline_actual = []\n",
    "baseline_pred = []\n",
    "for index, row in ratings_test.iterrows():\n",
    "    user = row['uID']\n",
    "    movie = row['mID']\n",
    "    actual_rating = row['rating']\n",
    "    baseline_actual.append(actual_rating)\n",
    "    baseline_pred.append(global_mean)\n",
    "\n",
    "baseline_rmse = np.sqrt(mean_squared_error(baseline_actual, baseline_pred))\n",
    "print(f\"Baseline Predictor RMSE: {baseline_rmse:.4f}\")\n"
   ],
   "id": "e18d429b838771a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Discussion for Part 2\n",
    "#\n",
    "# **Observations:**\n",
    "# - The RMSE of the NMF model varies with the number of latent factors. In some settings, the improvement over the simple global mean baseline is small.\n",
    "#\n",
    "# **Limitations of sklearn’s NMF:**\n",
    "# - **Sensitivity to Initialization:** Despite using NNDSVD, the model may converge to a suboptimal local minimum.\n",
    "# - **Handling Missing Data:** Filling missing ratings with 0 is not ideal since 0 can be misinterpreted as an actual rating, potentially biasing the factorization.\n",
    "# - **Sparsity Modeling:** The factorization does not explicitly model the sparse nature of the ratings matrix.\n",
    "#\n",
    "# **Potential Improvements:**\n",
    "# - Use multiple random restarts or more advanced initialization methods to improve convergence.\n",
    "# - Apply imputation techniques or use methods that inherently model missing data (e.g., probabilistic matrix factorization).\n",
    "# - Explore hybrid approaches that combine matrix factorization with baseline or similarity-based predictors.\n",
    "#\n",
    "# These improvements could lead to better performance in practice.\n",
    "\n",
    "# %% [markdown]\n",
    "# # End of Notebook\n"
   ],
   "id": "bf9a55d2fa673040"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
